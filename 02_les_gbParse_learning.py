# есл ирекурсивно идти по пегинации, то на большом ресукрсе будет переполнение.
# каждый переход на страницу и обработка - это одна задача. Всего 2 вида задач, пегинация и обработка каждой статьи
# задачи будем храниитьт в очереди fifo
# цикл будет идти по итерированному объекту ,брать оттуда задачу, выполнять ее и переходитьт к следующей.
# выполняемая задача может пополнить этот список (объект) новыми задачами

import typing
import time
import requests
import bs4
from urllib.parse import urljoin  # крутой умный конкатинатор url
# urljoin('https://gb.ru/posts', '/posts?page=2')  -> 'https://gb.ru/posts?page=2'
# urljoin('https://gb.ru/posts', 'https://gb.ru/posts?page=2')  -> 'https://gb.ru/posts?page=2'
import pymongo


class GdBlogParse:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0"
    }
    _parse_time = 0  # выгодно - мало весит

    def __init__(self, start_url, db, delay=1.0):
        self.start_url = start_url
        self.db = db
        self.delay = delay
        self.done_urls = set()  # будем сохранять url куда уже ходили, свойства set - уникальные элементы,
        # работает быстрее - более оптимизированно хранятся даннеые внутри, хеши, поиск быстрый, есть такое значение
        # или нет  (tuple - не добавтиь новое занчение, )
        self.tasks = []  # нужна очередь задачь,
        # в питоне конечно есть очереди и они позволяют быстро делать fifo or filo, но мы сделаем упрощенно на списке,
        # это неправильно в продашне, но в целом можно и так
        self.tasks_creator({self.start_url, }, self.parse_feed)  # создаем первую задачу, даем set url-ов и ф-ю обработки

    def _get_response(self, url):  # хочу чтобы глобально не делались запросы чаще определенного времени,
        # чаще чем это делается через delay,
        # для этого вводим приватный атрибут _parse_time
        # next_time = self._parse_time + self.delay   ===== перенесли вниз, атк как будет проблема времени
        while True:  # реализуем сон между запросами
            next_time = self._parse_time + self.delay
            if next_time > time.time():  # выгодно time.time() = float
                time.sleep(next_time - time.time())  # ексли будем делать _get_response когда вроемя еще не пришло,
                # то наш парсер уснет на время нужное, потом проснется и сделает запрос.
                # надо всегда учитывать время когда можно сделать запрос
                # сделав запрос - мы запоминаем время когда мы его сделали,
                # и каждый раз будем сверяться с этим внеменем,
                # вначале parse_time = 0 и значит время пришло всегда.
                # next_time не будет больше time.time (время от 1970 года в секундах)
                # если мы слишкоим быстро сделаем запрос, тоесть next_time > time.time() - то условие if нас сдержит
                # и уснет на разницу next_time - time.time()
            response = requests.get(url, headers=self.headers)
            print(f"RESPONSE: {response.url}")
            self._parse_time = time.time()  # фиксируем _parse_time после запроса, для задержки потом в if
    # респонз может обрабатываться и 5-10 секунд.Если просто задержку поставить то будет всегда задержка,
            # а с подсчетом как тут, более эффективно расходуется время, так как пауза будет только в том случае,
            # если запрособрабатывалося меньше delay

            if response.status_code == 200:  # статус код выгоден так как с числом сравниваем
                return response

    # методика замыкания, похож на декораторы
    # это же можно организовать с помощью снешних классов,
    # определить определенный колабл класс и использовать его в виде задач
    def get_task(self, url: str, callback: typing.Callable) -> typing.Callable: # передаем url и  ф-ю которая должна яего обработать
        # хотим обрабатывать без рекурсий, имеем обэект обработчик, и откладываем задачи на потом
        def task():  # ничего не принимает но делает важную вещь,
            response = self._get_response(url)  # тут мы должны получить некий респонз и передать его в колбэк
            return callback(response)  # ретерним колбэк и выполняем его, передав туда респонз
        return task
    # когда указываешь тип url например, ide подсказывает - чего оти меня ожидает ф-я,
    # плюс линтеры могут показывать потенциальные ошибки в вашем коде, безопасность кода так лучше обеспечивается
    # СТРОГАЯ ТИПИЗАЦИЯ если будете контролировать каждый тип и знать где что происходит, лучше чем динамич.
    # динамич только изза того что она есть используете?

    def tasks_creator(self, urls: set, callback: typing.Callable):  # ничего не возвращаем
        # нужно вычислить те url которые обработаны и удалить из списка url и после этого насоздавать задач
        urls_set = urls - self.done_urls  # вычитаем мноджества, оставляем urls которые не входят в done_urls
        for url in urls_set:
            self.tasks.append(
                self.get_task(url, callback)
            )
            self.done_urls.add(url)  # добавляем в сделано, так как задачу создали по этому url



    def run(self):  # будет просто запускать цикл
        # # как ссылки извоекать? создадим первую задачу - это можно сджелать в разных местах (в ините можно, тут можно)
        # self.tasks.append(self.get_task(self.start_url, self.parse_feed))  # старт url будет обрабатываться feed
        # # результат работы get_task это ф-я task !!!!!!
        # # для определенной task существует в ее области видимости url и метод (в даннном случае parse_feed)
        # self.done_urls.add(self.start_url)  # если задача поставлена - url утилизирован
        # потом вынесли верхнее в инит, но в tasks_creator - и run стыл чистый

        while True:
            try:
                task = self.tasks.pop(0)  # извлекаем из очережи задачу под 0 индексом,
                # так как она колабл мы ее вызываем ниже
                task()  # и он будет работать, вызывыаем его на обработку задачи,
                # при ее вызове мы попаданем в def task, для него существует определенный url и опред callback
                # на этот url мы делаем запрос, получаем запрос и передаем его в колбэк,
                # которым является parse_feed (в данном случае), а он должен извлечь ссылки и породить новые таски аппендить
            except IndexError:  # исключение мб только если список задач пустой
                break
            # отлавливать исключение выгоднее, чем постоянно мерить длину списка
            # у нас есть ошибка которую мы ждем, это быстрее, знаем как на нее отреагировать, не надо if-else
            # сложность алгоритма проще

    # теперь нудны всего 2 обработчика задач, обрабатывать страницы со списком постов, и пегинацией
    # страница ленты и страница самого поста - это два разных обработчика, если делать единым обработчиком
    # - это будут сложные условия
    # dry - don't repeat yourself, solid - заставляет нас декомпозировать задачу
    # вот есть задача обрабатывать запросы с учетом времени get_response
    # есть задача пораждать таски - у нас етсь метод get_task
    # можно было в один? да, но плдохо было бы
    def parse_feed(self, response: requests.Response,  ):  # собирает из респонза все ссылки на пегинацию и посты,
                                                            # пораждать задачи, будет пополнять tasks (в виде колбл объектов)
        soup = bs4.BeautifulSoup(response.text, 'lxml')  # по умолчанию html.parse
        # BS не умеет делать запросы, не умеет работать с хедерами, может обрабатывать только текстовую информацию
        # ему надо передавать html или xml в виде текстовой строки
        # можно скачаать весь сайт ссылки html а потом его обработать c помощбю BS
        ul_pagination = soup.find('ul', attrs={"class": "gb__pagination"})
        pagination_links = set(
            urljoin(response.url, itm.attrs.get('href')) for itm in ul_pagination.find_all('a') if itm.attrs.get('href')
        )
        # itm.attrs.get('href') for itm in ul_pagination.find_all('a') if itm.attrs.get('href') - было так
        # {'/posts?page=2', '/posts?page=3', '/posts?page=57'} а стали полные ссылки

        # щас нам нужно породить задачу, у нас относительбные url в pagination_links
        self.tasks_creator(pagination_links, self.parse_feed)
        post_wrapper = soup.find("div", attrs={"class": "post-items-wrapper"})
        post_links = set(
            urljoin(response.url, itm.attrs.get('href'))
            for itm in post_wrapper.find_all("a", attrs={"class": "post-item__title"})
            if itm.attrs.get('href')
        )
        self.tasks_creator(post_links, self.parse_post)

        # можно так заменить верхнее, post_links убрать, но такой код сложне некоторым читать
        # self.tasks_creator(
        #     set(
        #         urljoin(response.url, itm.attrs.get('href'))
        #         for itm in post_wrapper.find_all("a", attrs={"class": "post-item__title"})
        #         if itm.attrs.get('href')
        #     ),
        #     self.parse_post
        # )

    def parse_post(self, response: requests.Response):  # долджен обрадатывать страницу поста и извлекать из него данные
        soup = bs4.BeautifulSoup(response.text, 'lxml')
        author_name_tag = soup.find('div', attrs={"itemprop": "author"})
        data = {
            'url': response.url,
            "title": soup.find('h1', attrs={'class': 'blogpost-title'}).text,  # делали в консоли, из soup... при дебаге
            # хотим имя автора и ссылку на автора
            "author": {
                'url': urljoin(response.url, author_name_tag.parent.attrs['href']),
                'name': author_name_tag.text
            }
            # обработку более сложных структур можно вынести в отдельные методы
        }
        self._save(data)

    def _save(self, data: dict):
        collection = self.db["gb_blog_parse"]  # пришем не в тблицу а в коллекцию, которая внутри бд
        # при этом ее там физически не существует,
        # но база и коллекция появится тогда когда произойдет первая транзакция в нее.
        # будет создана как только попытаемся в нее писать
        collection.insert_one(data)  # insert_one принимает в себя экземпляр класса словаря,
        # либо словарно совместимый тип (но ключи обязательно строчный тип должны иметь )
        # collection.insert_one({"hello": 1, "pass": 2}) - так можно вручную добавить в консоли, где "hello" строка!!!
        # collection.insert_one({"hello": 1, "pass": datetime.datetime.now()}) и типа datetime можно сохранять
        # и когда назад его получу, ьубу иметь эеземпляр класса datetime !!!! в обе стороны работает
        # insert_many принимает список словарей
        # insert_one и insert_many изменяют тип данных,
        # еслои named_tuple (похожий на словарь, но является неизменяем), тоего нельзя тут использовать
        # придется использовать изменяемый тип
        print(1)


if __name__ == '__main__':
    client_db = pymongo.MongoClient()  # создаем клиент БД, если локальным сервером пользуетесь, то так достаточно
    # но стоит указывать строку коннекта
    # client_db = pymongo.MongoClient("mongodb://localhost:27017") # но это не полный синтаксис, а может быть еще ...
    # "mongodb://localhost:27017" - это дефолтное значение и можно ничего не указывать
    # client_db = pymongo.MongoClient("mongodb://user:password@ip(domain_name, mongo_server_address, mongo_server_port)localhost:27017/database_name")

    # мы подключились только к серверу - у нас открыт клиент, экземпляр класса клиент ,соединений пока не было
    # в sql структура хранения = сервер sql=> база данных => таблица => строка данных в таблице
    # нельзя в таблице sql хранить данные разные по структуре, две записи с разными именами колонок - нельязя хранить
    # в monbogb = сервер sql=> база данных => коллекция => в коллекции содердится документ
    # в одной коллекции можно хранить (главное чтобы это был словарь внешне), внутри мб все что угодно
    # могут не сходиться имена ключей, какие то данные могут отсутствовать.
    # преимущества nosql (высокая запись транзакции и чтения) -
    # sql базы - пытается хранить все данные в оперативной памяти, + скорость высокая, минус что это в оперативке,
    # и если сбой - то данные будут потеряны.
    # Можно скидывать на диск данные и период времени, но это будет замедлять работу
    # "все или ничего" в sql = атомарность, если чтото не срабатывает то ничего не срабатывает
    # - это дает безопасность транзакций - поэтому sql используется в банках,
    # в рамках одной транзакции с вашего счета списываются средства и на другой счет зачисляются.
    # если не смогут списаться то не смогут записаться другому, Если не запишутся другому, то и у вас не спишутся
    # поэтому внутрибанковские транзакции ничего не стоили, а межбанковские стоили больше денег, комиссия взимается
    # mongodb этого не обещает, в нем нет журнала событий как в sql,
    # но делает быстро и обрабатывает миллиона зыписей в секунду, при малом потреблении ресурсов.
    # еще + nosql масштабируемость (sql горизонтально плохо масштабируются, нужны круте инженеры - вертикально нормально)
    # вертикально - это увеличить мощность сервера, оперативка процб ssd raid. У этого есть низкий потолок,
    # нельзя бесконечно вставлять оперутивку, так как процы ограничены набором.
    # горизонтально - рядом ставят сервер и они работают как один. Объединили их в кластер.
    # mongo легко - перехватывать будут сервера друг у друга запросы.
    # Postgre mysql - такое не прокатит. Там надо долго конфигурировать сервер.
    # Твиттер в 2015 году пытался аерейти на mongodb (или какуюто nosql) неполучмлось, безопасномсти не хватает
    # sql позволяет быстро откатыватсья, подниматься по транзакциям и тд
    # nosql этго не дает, но быстрый и поэтому в датамайнинге прижился, и в датасайенс -
    # тем что с ним очень быстро можно начать рабортать и писать.
    # Чтоыб начать пистаь в sql - надо определить структуру таблиц, типы полей, т.е. задекларировать бд,

    db = client_db['db_parse_19_05']  # это название базы данных = db_parse_19_05
    # говорим что мы хотим работать с базой данныз которая находится в клиенте, обращаемся через интерфейс словаря
    # теперь у нас етсь коннектор к бд, передадим его в парсер, соединений с бд еще не было
    parser = GdBlogParse('https://gb.ru/posts', db)
    parser.run()

# инсталлируем монго на комп
# pip install pymongo
# nosql база значит данные не в формате таблиц, где каждая запись это строка,
# а в формате документов которые кодируются с помощью bson, это json типа
# - первичная структура для работы с объектами mongo словарь мб только - это внешняя структура,
# если json может по внешней структуре быть хоть списком хоть чем
# вложенность уже имеет расширенные типы, datetime объект даже и будет корректно отрабатывать
# и потом искать по времени и даже по дельте времени. Координаты хранить можно,
# облачные бесплатные есть на heroku - хотя ограничена там, и на сайте монго бесплатная можно тоже с ограничениями
