# есл ирекурсивно идти по пегинации, то на большом ресукрсе будет переполнение.
# каждый переход на страницу и обработка - это одна задача. Всего 2 вида задач, пегинация и обработка каждой статьи
# задачи будем храниитьт в очереди fifo
# цикл будет идти по итерированному объекту ,брать оттуда задачу, выполнять ее и переходитьт к следующей.
# выполняемая задача может пополнить этот список (объект) новыми задачами

import typing

import time

import requests

class GdBlogParse:
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:88.0) Gecko/20100101 Firefox/88.0"
    }
    _parse_time = 0 # выгодно - мало весит

    def __init__(self, start_url, delay=1.0):
        self.start_url = start_url
        self.delay = delay
        self.done_urls = set()  # будем сохранять url куда уже ходили, свойства set - уникальные элементы,
        # работает быстрее - более оптимизированно хранятся даннеые внутри, хеши, поиск быстрый, есть такое значение
        # или нет  (tuple - не добавтиь новое занчение, )
        self.tasks = []  # нужна очередь задачь,
        # в питоне конечно есть очереди и они позволяют быстро делать fifo or filo, но мы сделаем упрощенно на списке,
        # это неправильно в продашне, но в целом можно и так

    def _get_response(self, url):  # хочу чтобы глобально не делались запросы чаще определенного времени,
                # чаще чем это делается через delay,
                # для этого вводим приватный атрибут _parse_time
        # next_time = self._parse_time + self.delay   ===== перенесли вниз, атк как будет проблема времени
        while True:  # реализуем сон между запросами
            next_time = self._parse_time + self.delay
            if next_time > time.time():  # выгодно time.time() = float
                time.sleep(next_time - time.time())  # ексли будем делать _get_response когда вроемя еще не пришло,
                                    # то наш парсер уснет на время нужное, потом проснется и сделает запрос.
                # надо всегда учитывать время когда можно сделать запрос
                # сделав запрос - мы запоминаем время когда мы его сделали, и каждый раз будем сверяться с этим внеменем,
                # вначале parse_time = 0 и значит время пришло всегда.
                # next_time не будет больше time.time (время от 1970 года в секундах)
                # если мы слишкоим быстро сделаем запрос, тоесть next_time > time.time() - то условие if нас сдержит
                # и уснет на разницу next_time - time.time()
                response = requests.get(url, headers=self.headers)
                self._parse_time = time.time()  # фиксируем _parse_time после запроса, для задержки потом в if
        # респонз может обрабатываться и 5-10 секунд.Если просто задержку поставить то будет всегда задержка,
                # а с подсчетом как тут, более эффективно расходуется время, так как пауза будет только в том случае,
                # если запрособрабатывалося меньше delay

                if response.status_code == 200:  # статус код выгоден так как с числом сравниваем
                    return response

    def # 44 минуты
